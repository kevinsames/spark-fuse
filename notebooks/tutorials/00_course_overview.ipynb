{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Spark Fuse Notebook Course\n",
        "\n",
        "Use this learning path to move from notebook fundamentals to advanced PySpark testing techniques. Work through the modules in order; each later notebook builds on the concepts introduced earlier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to Use This Course\n",
        "\n",
        "1. Open each notebook inside `notebooks/tutorials/` using Jupyter, VS Code, or Databricks.\n",
        "2. Run the cells sequentially, taking notes and adapting the examples to your environment.\n",
        "3. Revisit earlier notebooks whenever you need a refresher\u2014the linked summaries below explain what you will learn in each module."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Course Modules\n",
        "\n",
        "- [01_notebook_fundamentals.ipynb](01_notebook_fundamentals.ipynb) \u2014 Understand interactive notebook concepts, cell types, and typical workflows.\n",
        "- [02_python_essentials_for_pyspark.ipynb](02_python_essentials_for_pyspark.ipynb) \u2014 Refresh core Python patterns (collections, functions, pathlib) that underpin PySpark code.\n",
        "- [03_getting_started_with_pyspark.ipynb](03_getting_started_with_pyspark.ipynb) \u2014 Spin up a SparkSession, build DataFrames, and perform foundational transformations.\n",
        "- [04_spark_sql_vs_pyspark.ipynb](04_spark_sql_vs_pyspark.ipynb) \u2014 Compare equivalent logic expressed with the DataFrame API and Spark SQL.\n",
        "- [05_spark_dataframe_joins.ipynb](05_spark_dataframe_joins.ipynb) \u2014 Practice inner, left, and broadcast joins using a shared dataset.\n",
        "- [06_spark_window_functions.ipynb](06_spark_window_functions.ipynb) \u2014 Apply running totals, rolling averages, and ranking with window specifications.\n",
        "- [07_testing_pyspark_workflows.ipynb](07_testing_pyspark_workflows.ipynb) \u2014 Learn pragmatic strategies for validating PySpark logic with assertions and pytest.\n",
        "- [08_advanced_testing_with_pyspark_testing.ipynb](08_advanced_testing_with_pyspark_testing.ipynb) \u2014 Leverage `pyspark.testing` utilities for robust DataFrame comparisons and schema checks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Shared Resources\n",
        "\n",
        "All notebooks rely on the demo dataset stored at `notebooks/data/orders_demo.csv`. The examples infer schema on load, so you can extend the dataset or swap in your own sample data as you progress."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "- Sketch a learning schedule for the course and note which datasets or cluster resources you'll need for each module.\n",
        "- Identify which existing projects could benefit from the concepts in each notebook.\n",
        "- Share the overview with your team and gather additional topics they'd like to see covered.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
