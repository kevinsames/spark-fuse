{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Joining DataFrames with Spark\n",
        "\n",
        "Learn how to combine multiple DataFrames in PySpark using different join strategies while keeping data lineage clear."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Start or reuse a SparkSession. We'll continue using the shared orders dataset stored under `notebooks/data/orders_demo.csv`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from pyspark.sql import SparkSession, functions as F\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkJoinsTutorial').getOrCreate()\n",
        "\n",
        "repo_root = Path.cwd()\n",
        "if (repo_root / 'notebooks').exists():\n",
        "    orders_path = repo_root / 'notebooks' / 'data' / 'orders_demo.csv'\n",
        "else:\n",
        "    orders_path = Path('..') / 'data' / 'orders_demo.csv'\n",
        "\n",
        "orders_df = (\n",
        "    spark.read\n",
        "    .option('header', True)\n",
        "    .option('inferSchema', True)\n",
        "    .csv(str(orders_path))\n",
        ")\n",
        "orders_df.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Dimension Data\n",
        "\n",
        "Joins require related keys. Here we create a simple region dimension with additional attributes to enrich the orders dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "region_info = [\n",
        "    ('east', 'East Coast', 'EST'),\n",
        "    ('north', 'Northern Region', 'CST'),\n",
        "    ('south', 'Southern Region', 'CST'),\n",
        "    ('west', 'Western Region', 'PST'),\n",
        "]\n",
        "columns = ['region', 'region_name', 'timezone']\n",
        "regions_df = spark.createDataFrame(region_info, columns)\n",
        "regions_df.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inner Join\n",
        "\n",
        "An inner join returns rows with matching keys in both DataFrames."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "inner_joined = orders_df.join(regions_df, on='region', how='inner')\n",
        "inner_joined.orderBy('order_date', 'region').show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Left Join\n",
        "\n",
        "A left join keeps all rows from the left DataFrame and fills unmatched lookups with nulls."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "left_joined = orders_df.join(regions_df, on='region', how='left')\n",
        "left_joined.orderBy('order_date', 'region').show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Handling Missing Matches\n",
        "\n",
        "If a region appears in the orders data but not in the dimension table, joins expose nulls. Let's simulate this by adding a new region to the orders data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "augmented_orders_df = orders_df.unionByName(\n",
        "    spark.createDataFrame([('2024-01-04', 'central', 8)], orders_df.columns)\n",
        ")\n",
        "left_with_missing = augmented_orders_df.join(regions_df, on='region', how='left')\n",
        "left_with_missing.orderBy('order_date', 'region').show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Broadcast Joins\n",
        "\n",
        "When the dimension table is small, broadcasting it avoids shuffle and speeds up joins."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "broadcast_joined = orders_df.join(F.broadcast(regions_df), on='region', how='inner')\n",
        "broadcast_joined.explain(mode='formatted')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean Up\n",
        "\n",
        "Stop the SparkSession to release resources when you finish experimenting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "- Create a small product dimension DataFrame and join it to the orders dataset using an inner join.\n",
        "- Demonstrate a left anti join to identify regions that appear in the dimension table but not in the orders data.\n",
        "- Convert one of the joins to use an explicit join condition involving multiple columns (e.g., region plus day part).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
