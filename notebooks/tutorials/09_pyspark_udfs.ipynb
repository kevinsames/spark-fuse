{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building and Using PySpark UDFs\n",
        "\n",
        "Explore user-defined functions (UDFs) in PySpark, understand when to reach for them, and see how to keep performance and testing in check.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## When to Use UDFs\n",
        "\n",
        "- Express custom business logic that is hard to model using built-in functions.\n",
        "- Integrate external libraries or Python-only algorithms into Spark pipelines.\n",
        "- Prototype transformations quickly before rewriting them with Spark SQL functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Shared Dataset\n",
        "\n",
        "Load the shared orders dataset we use throughout the course so we can attach UDF-generated columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from pyspark.sql import SparkSession, functions as F, types as T\n",
        "\n",
        "spark = SparkSession.builder.appName('PySparkUDFsTutorial').getOrCreate()\n",
        "\n",
        "repo_root = Path.cwd()\n",
        "if (repo_root / 'notebooks').exists():\n",
        "    data_path = repo_root / 'notebooks' / 'data' / 'orders_demo.csv'\n",
        "else:\n",
        "    data_path = Path('..') / 'data' / 'orders_demo.csv'\n",
        "\n",
        "orders_df = (\n",
        "    spark.read\n",
        "    .option('header', True)\n",
        "    .option('inferSchema', True)\n",
        "    .csv(str(data_path))\n",
        "    .withColumn('order_date', F.to_date('order_date'))\n",
        ")\n",
        "orders_df.orderBy('order_date', 'region').show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scalar UDF Basics\n",
        "\n",
        "A scalar UDF transforms each row independently. Use `pyspark.sql.functions.udf` with a return type to register it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "\n",
        "@udf(returnType=T.StringType())\n",
        "def classify_region(region: str) -> str:\n",
        "    if region in {'east', 'west'}:\n",
        "        return 'coastal'\n",
        "    return 'inland'\n",
        "\n",
        "with_udf = orders_df.withColumn('region_type', classify_region(F.col('region')))\n",
        "with_udf.orderBy('order_date', 'region').show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pandas UDF for Vectorized Logic\n",
        "\n",
        "Pandas UDFs (a.k.a. vectorized UDFs) operate on batches of data for better performance. They require Arrow to be enabled and return pandas objects.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import pandas_udf\n",
        "\n",
        "@pandas_udf('double')\n",
        "def add_priority_bonus(orders):\n",
        "    return orders * 1.1\n",
        "\n",
        "with_bonus = with_udf.withColumn('orders_with_bonus', add_priority_bonus(F.col('orders')))\n",
        "with_bonus.orderBy('order_date', 'region').show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## When to Prefer Built-ins Over UDFs\n",
        "\n",
        "- Built-in functions benefit from Catalyst optimization; they are usually faster.\n",
        "- Use UDFs only when necessary\u2014consider rewriting a UDF as SQL functions or window expressions once logic stabilizes.\n",
        "- Watch out for serialization overhead: Pandas UDFs mitigate some cost but still require careful benchmarking.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing UDFs\n",
        "\n",
        "Combine Python unit tests with DataFrame comparisons. Here we reuse the testing helpers from earlier tutorials.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pyspark.testing.utils import assertDataFrameEqual\n",
        "\n",
        "expected_rows = [\n",
        "    ('east', 'coastal'),\n",
        "    ('north', 'inland'),\n",
        "    ('south', 'inland'),\n",
        "]\n",
        "expected_df = spark.createDataFrame(expected_rows, schema=['region', 'region_type']).orderBy('region')\n",
        "actual_df = with_udf.select('region', 'region_type').distinct().orderBy('region')\n",
        "\n",
        "assertDataFrameEqual(actual_df, expected_df)\n",
        "print('Region classification UDF behaves as expected.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "- Implement a scalar UDF that categorizes `orders` into buckets such as `low`, `medium`, and `high`.\n",
        "- Write a Pandas UDF that normalizes orders within each region and compare it against a built-in window function approach.\n",
        "- Benchmark a UDF against a built-in function using `timeit` or Spark\u2019s event logs to understand the performance trade-offs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "spark.stop()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
