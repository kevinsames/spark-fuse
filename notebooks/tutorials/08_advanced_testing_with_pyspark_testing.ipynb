{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced Testing with pyspark.testing\n",
        "\n",
        "Use the built-in `pyspark.testing` helpers to assert DataFrame equality, schemas, and structured transformations with minimal boilerplate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "- Spark environment capable of running PySpark notebooks.\n",
        "- Shared dataset `notebooks/data/orders_demo.csv` available.\n",
        "- PySpark 3.5 or later (includes the `pyspark.testing.utils` module).\n",
        "- `setuptools` installed (provides the `distutils` module required by PySpark 3.5 on Python 3.12).\n",
        "- NumPy 2.0+ users should alias `np.NaN` to `np.nan` (see helper cell below).\n",
        "- Optional: pytest for running automated suites.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Ensure distutils is available (Python 3.12 requires setuptools to provide it)\n",
        "try:\n",
        "    import distutils  # type: ignore\n",
        "except ModuleNotFoundError as exc:\n",
        "    raise ModuleNotFoundError(\"The built-in pyspark.testing helpers require distutils. Install setuptools with `pip install setuptools`.\") from exc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Session and Data Setup\n",
        "\n",
        "We reuse the orders demo dataset and create a view for convenience.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from pyspark.sql import SparkSession, functions as F, types as T\n",
        "\n",
        "spark = SparkSession.builder.appName('AdvancedPySparkTesting').getOrCreate()\n",
        "\n",
        "repo_root = Path.cwd()\n",
        "if (repo_root / 'notebooks').exists():\n",
        "    data_path = repo_root / 'notebooks' / 'data' / 'orders_demo.csv'\n",
        "else:\n",
        "    data_path = Path('..') / 'data' / 'orders_demo.csv'\n",
        "\n",
        "orders_df = (\n",
        "    spark.read\n",
        "    .option('header', True)\n",
        "    .option('inferSchema', True)\n",
        "    .csv(str(data_path))\n",
        "    .withColumn('order_date', F.to_date('order_date'))\n",
        ")\n",
        "orders_df.createOrReplaceTempView('orders')\n",
        "orders_df.orderBy('order_date', 'region').show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transformation Under Test\n",
        "\n",
        "Here we implement a reusable helper that aggregates daily orders per region with both sums and averages. The function will be the subject of our assertions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def summarize_orders(df):\n",
        "    \"\"\"Aggregate total and average orders per region.\"\"\"\n",
        "    return (\n",
        "        df.groupBy('region')\n",
        "          .agg(\n",
        "              F.sum('orders').alias('total_orders'),\n",
        "              F.avg('orders').alias('avg_orders'),\n",
        "          )\n",
        "          .orderBy('region')\n",
        "    )\n",
        "\n",
        "summary_df = summarize_orders(orders_df)\n",
        "summary_df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Ensure NumPy compatibility: NumPy 2.0 removed np.NaN alias used by pyspark.testing\n",
        "import numpy as np\n",
        "if not hasattr(np, 'NaN'):\n",
        "    np.NaN = np.nan\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Asserting DataFrame Equality\n",
        "\n",
        "`pyspark.testing.utils.assertDataFrameEqual` normalizes Spark schemas so you can compare DataFrames directly without hand-written sorting or casting. Ensure your expected schema matches the nullability Spark produces, because the helper validates full schema equality.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pyspark.testing.utils import assertDataFrameEqual\n",
        "\n",
        "expected_rows = [\n",
        "    ('east', 30, 10.0),\n",
        "    ('north', 34, 34 / 3),\n",
        "    ('south', 39, 13.0),\n",
        "]\n",
        "expected_df = spark.createDataFrame(expected_rows, schema=summary_df.schema).orderBy('region')\n",
        "\n",
        "assertDataFrameEqual(summary_df, expected_df)\n",
        "print('DataFrame contents match expected totals and averages.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing Schemas Explicitly\n",
        "\n",
        "Schema comparisons prevent accidental type drift. Use `assertSchemaEqual` from `pyspark.testing.utils` to ensure fields stay aligned.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pyspark.testing.utils import assertSchemaEqual\n",
        "\n",
        "assertSchemaEqual(summary_df.schema, expected_df.schema)\n",
        "print('Schemas match the expected structure.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing with pytest + pyspark.testing\n",
        "\n",
        "Combine these assertions with pytest fixtures for automated suites. Example layout:\n",
        "\n",
        "```python\n",
        "# tests/conftest.py\n",
        "import pytest\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "@pytest.fixture(scope='session')\n",
        "def spark():\n",
        "    session = (\n",
        "        SparkSession.builder\n",
        "        .appName('pytest-pyspark')\n",
        "        .master('local[2]')\n",
        "        .getOrCreate()\n",
        "    )\n",
        "    yield session\n",
        "    session.stop()\n",
        "```\n",
        "\n",
        "```python\n",
        "# tests/test_summarize_orders.py\n",
        "from pyspark.testing.utils import assertDataFrameEqual\n",
        "from myproject.transforms import summarize_orders\n",
        "\n",
        "def load_orders(spark):\n",
        "    return (\n",
        "        spark.read\n",
        "        .option('header', True)\n",
        "        .option('inferSchema', True)\n",
        "        .csv('notebooks/data/orders_demo.csv')\n",
        "    )\n",
        "\n",
        "def test_summarize_orders_totals(spark):\n",
        "    df = load_orders(spark)\n",
        "    result = summarize_orders(df)\n",
        "\n",
        "    expected = spark.createDataFrame([\n",
        "        ('east', 30, 10.0),\n",
        "        ('north', 34, 34 / 3),\n",
        "        ('south', 39, 13.0),\n",
        "    ], ['region', 'total_orders', 'avg_orders']).orderBy('region')\n",
        "\n",
        "    assertDataFrameEqual(result.orderBy('region'), expected, check_nullable=False)\n",
        "```\n",
        "\n",
        "`pyspark.testing.utils` handles Spark-specific comparison logic, keeping your tests concise.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Snapshot Testing Wide DataFrames\n",
        "\n",
        "For larger comparisons, serialize sorted results to JSON files checked into version control, then use `assertDataFrameEqual` against the reloaded snapshot. Update snapshots intentionally when business logic changes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean Up\n",
        "\n",
        "Stop the SparkSession when you finish working in the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "- Write a new aggregation helper (for example, daily averages) and test it with `assertDataFrameEqual`.\n",
        "- Use `assertSchemaEqual` to ensure a schema change is detected when you add a nullable column.\n",
        "- Combine `assertDataFrameEqual` with `assertColumnEquality` (from the same module) to verify both the structure and values of a transformation.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
