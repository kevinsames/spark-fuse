{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing PySpark Workflows\n",
        "\n",
        "Learn strategies for validating PySpark logic with assertions and pytest-style tests while working with the shared demo dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why Test PySpark Code?\n",
        "\n",
        "- Catch logical regressions in complex transformations early.\n",
        "- Document expected data contracts for collaborators.\n",
        "- Build confidence before shipping notebooks into production pipelines.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the Shared Dataset and SparkSession\n",
        "\n",
        "We reuse `notebooks/data/orders_demo.csv` to keep examples consistent with other tutorials.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from pyspark.sql import SparkSession, functions as F\n",
        "\n",
        "spark = SparkSession.builder.appName('TestingPySparkTutorial').getOrCreate()\n",
        "\n",
        "repo_root = Path.cwd()\n",
        "if (repo_root / 'notebooks').exists():\n",
        "    data_path = repo_root / 'notebooks' / 'data' / 'orders_demo.csv'\n",
        "else:\n",
        "    data_path = Path('..') / 'data' / 'orders_demo.csv'\n",
        "\n",
        "orders_df = (\n",
        "    spark.read\n",
        "    .option('header', True)\n",
        "    .option('inferSchema', True)\n",
        "    .csv(str(data_path))\n",
        "    .withColumn('order_date', F.to_date('order_date'))\n",
        ")\n",
        "orders_df.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function Under Test\n",
        "\n",
        "Create a transformation helper so we can assert on its output. Here, we tag demand levels by threshold.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def tag_demand_level(df, threshold=14):\n",
        "    \"\"\"Classify orders by demand level using the provided threshold.\"\"\"\n",
        "    return df.withColumn(\n",
        "        'demand_level',\n",
        "        F.when(F.col('orders') >= threshold, 'high').otherwise('steady')\n",
        "    )\n",
        "\n",
        "tagged_df = tag_demand_level(orders_df)\n",
        "tagged_df.orderBy('order_date', 'region').show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inline Assertions Inside Notebooks\n",
        "\n",
        "For quick experiments, use Python's built-in `assert` statements to validate expectations directly in the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Ensure every region has at least one row labelled steady\n",
        "steady_counts = (\n",
        "    tagged_df\n",
        "      .filter(F.col('demand_level') == 'steady')\n",
        "      .groupBy('region')\n",
        "      .count()\n",
        "      .collect()\n",
        ")\n",
        "assert len(steady_counts) == 3, 'Expected steady rows for each region'\n",
        "print('All regions include steady demand rows.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing DataFrames Deterministically\n",
        "\n",
        "Collecting ordered tuples keeps comparisons deterministic. This pattern is handy when writing unit tests without helper libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "expected = [\n",
        "    ('2024-01-01', 'north', 'steady'),\n",
        "    ('2024-01-02', 'north', 'high'),\n",
        "    ('2024-01-03', 'north', 'steady'),\n",
        "]\n",
        "\n",
        "actual = (\n",
        "    tagged_df\n",
        "      .filter(F.col('region') == 'north')\n",
        "      .orderBy('order_date')\n",
        "      .select('order_date', 'region', 'demand_level')\n",
        "      .rdd.map(lambda row: (row.order_date.strftime('%Y-%m-%d'), row.region, row.demand_level))\n",
        "      .collect()\n",
        ")\n",
        "assert actual == expected, f\"Unexpected demand levels: {actual}\"\n",
        "print('North region demand levels match the expectation.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Structuring Tests with pytest\n",
        "\n",
        "Move assertions into test modules to automate checks. Create a reusable Spark fixture and call transformation helpers. Example structure:\n",
        "\n",
        "```python\n",
        "# conftest.py\n",
        "import pytest\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "@pytest.fixture(scope='session')\n",
        "def spark():\n",
        "    session = (\n",
        "        SparkSession.builder\n",
        "        .appName('pytest-pyspark')\n",
        "        .master('local[2]')\n",
        "        .getOrCreate()\n",
        "    )\n",
        "    yield session\n",
        "    session.stop()\n",
        "```\n",
        "\n",
        "```python\n",
        "# tests/test_tag_demand.py\n",
        "from pathlib import Path\n",
        "from pyspark.sql import functions as F\n",
        "from myproject.transforms import tag_demand_level\n",
        "\n",
        "def load_orders(spark):\n",
        "    data_path = Path('notebooks/data/orders_demo.csv')\n",
        "    return (\n",
        "        spark.read\n",
        "        .option('header', True)\n",
        "        .option('inferSchema', True)\n",
        "        .csv(str(data_path))\n",
        "    )\n",
        "\n",
        "def test_tag_demand_level_high_classification(spark):\n",
        "    df = load_orders(spark)\n",
        "    result = tag_demand_level(df, threshold=14)\n",
        "    high_rows = result.filter((F.col('region') == 'south') & (F.col('demand_level') == 'high'))\n",
        "    assert high_rows.count() == 1\n",
        "```\n",
        "\n",
        "Run the suite with `pytest tests/` to execute all checks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Snapshot Testing DataFrames\n",
        "\n",
        "For larger transformations, compare sorted DataFrames or write helper utilities that normalize column order, null handling, and types before asserting equality.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean Up\n",
        "\n",
        "Stop the SparkSession at the end of notebook execution.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "- Add an assertion that verifies no region has negative order counts after running `tag_demand_level`.\n",
        "- Refactor the inline assertions into pytest-style `test_` functions and run them from the command line.\n",
        "- Introduce an intentional bug in `tag_demand_level` and observe how the tests fail, then fix the bug.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
