{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Getting Started with PySpark\n",
        "\n",
        "This tutorial walks through the essentials of PySpark\u2014the Python API for Apache Spark\u2014from creating a session to running transformations and SQL queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "- A Python environment with the `pyspark` package available.\n",
        "- Access to a Spark cluster or a local installation (the default session builder will create a local Spark instance).\n",
        "- Familiarity with basic Python data structures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Start or reuse a SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName('PySparkTutorial')\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a DataFrame\n",
        "\n",
        "PySpark DataFrames are distributed tables with named columns. You can create them from Python objects, files, or external systems. Here we build one from an in-memory list of tuples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Build a shared demo dataset\n",
        "from pathlib import Path\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "repo_root = Path.cwd()\n",
        "if (repo_root / 'notebooks').exists():\n",
        "    data_path = repo_root / 'notebooks' / 'data' / 'orders_demo.csv'\n",
        "else:\n",
        "    data_path = Path('..') / 'data' / 'orders_demo.csv'\n",
        "\n",
        "df = (\n",
        "    spark.read\n",
        "    .option('header', True)\n",
        "    .option('inferSchema', True)\n",
        "    .csv(str(data_path))\n",
        ")\n",
        "\n",
        "df.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transformations\n",
        "\n",
        "Transformations build a logical plan\u2014Spark executes them lazily when an action (like `show`) runs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Filter and enrich the DataFrame\n",
        "north_orders = df.filter(F.col('region') == 'north')\n",
        "with_levels = (\n",
        "    north_orders\n",
        "      .withColumn(\n",
        "          'demand_level',\n",
        "          F.when(F.col('orders') >= 14, 'high').otherwise('steady'),\n",
        "      )\n",
        "      .orderBy('order_date')\n",
        ")\n",
        "with_levels.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Aggregations\n",
        "\n",
        "Grouping and aggregation reveal high-level trends across large datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Summaries by region\n",
        "summary = (\n",
        "    df.groupBy('region')\n",
        "      .agg(\n",
        "          F.sum('orders').alias('total_orders'),\n",
        "          F.avg('orders').alias('avg_orders'),\n",
        "      )\n",
        "      .orderBy('region')\n",
        ")\n",
        "summary.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Spark SQL\n",
        "\n",
        "Spark lets you mix SQL queries with the DataFrame API. Register a temporary view and run SQL statements directly from Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "df.createOrReplaceTempView('orders')\n",
        "spark.sql(\n",
        "    '''\n",
        "    SELECT order_date,\n",
        "           region,\n",
        "           orders,\n",
        "           CASE WHEN orders >= 14 THEN 'high' ELSE 'steady' END AS demand_level\n",
        "    FROM orders\n",
        "    WHERE order_date = '2024-01-02'\n",
        "    ORDER BY region\n",
        "    '''\n",
        ").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean Up\n",
        "\n",
        "Stop the SparkSession when you are finished with the notebook to release resources."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "- Load the shared orders dataset, then add a new column that marks weekends versus weekdays.\n",
        "- Use `groupBy` to compute the maximum and minimum daily orders per region.\n",
        "- Write a short Markdown summary explaining how you would adapt the dataset loader to read from a data lake path.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
