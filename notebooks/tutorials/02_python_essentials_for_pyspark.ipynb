{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Python Essentials for PySpark Users\n",
        "\n",
        "PySpark applications are written in Python, so fluency with core language patterns makes your Spark code easier to read and maintain. This tutorial highlights Python concepts you will lean on while building PySpark jobs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "- Basic familiarity with Python syntax (`if`, `for`, functions).\n",
        "- Spark environment capable of running PySpark notebooks.\n",
        "- Access to the shared `orders_demo.csv` dataset under `notebooks/data/`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Python Collections to Stage Data\n",
        "\n",
        "Lists and dictionaries are common when assembling small lookup tables or configuration that feeds into Spark jobs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Create a list of dictionaries describing regions\n",
        "regions = [\n",
        "    {\"region\": \"north\", \"timezone\": \"CST\"},\n",
        "    {\"region\": \"south\", \"timezone\": \"CST\"},\n",
        "    {\"region\": \"east\", \"timezone\": \"EST\"},\n",
        "    {\"region\": \"west\", \"timezone\": \"PST\"},\n",
        "]\n",
        "regions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### List Comprehensions\n",
        "\n",
        "You can reshape Python collections concisely with comprehensions before handing them to Spark.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Add a label to each region using a comprehension\n",
        "labeled_regions = [\n",
        "    {**entry, \"label\": f\"{entry['region'].title()} Region\"} for entry in regions\n",
        "]\n",
        "labeled_regions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading Data with Path Helpers\n",
        "\n",
        "Python's `pathlib` is handy for pointing Spark to datasets without hard-coding absolute paths.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from pyspark.sql import SparkSession, functions as F\n",
        "\n",
        "spark = SparkSession.builder.appName('PythonForPySpark').getOrCreate()\n",
        "\n",
        "repo_root = Path.cwd()\n",
        "if (repo_root / 'notebooks').exists():\n",
        "    data_path = repo_root / 'notebooks' / 'data' / 'orders_demo.csv'\n",
        "else:\n",
        "    data_path = Path('..') / 'data' / 'orders_demo.csv'\n",
        "\n",
        "orders_df = (\n",
        "    spark.read\n",
        "    .option('header', True)\n",
        "    .option('inferSchema', True)\n",
        "    .csv(str(data_path))\n",
        ")\n",
        "orders_df.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mapping Python Functions to Transform Data\n",
        "\n",
        "Before creating Spark DataFrames, you may need to normalize raw Python lists. Functions keep that logic reusable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def annotate_region(entry):\n",
        "    'Return a tuple that Spark can ingest, tagging coastal regions.'\n",
        "    coastal = entry['region'] in {'east', 'west'}\n",
        "    return entry['region'], entry['timezone'], coastal\n",
        "\n",
        "region_tuples = [annotate_region(item) for item in labeled_regions]\n",
        "region_tuples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating DataFrames from Python Objects\n",
        "\n",
        "Once your Python data is structured, `createDataFrame` turns it into a Spark DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "schema = ['region', 'timezone', 'is_coastal']\n",
        "regions_df = spark.createDataFrame(region_tuples, schema)\n",
        "regions_df.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Combining Python Logic with Spark SQL Functions\n",
        "\n",
        "Use Python conditionals to choose Spark expressions dynamically, keeping complex business rules readable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def demand_category_expression(threshold):\n",
        "    if threshold < 12:\n",
        "        # For lower thresholds, treat anything above as elevated demand\n",
        "        return F.when(F.col('orders') > threshold, 'high').otherwise('steady')\n",
        "    return F.when(F.col('orders') >= threshold, 'high').otherwise('steady')\n",
        "\n",
        "demand_expr = demand_category_expression(threshold=13)\n",
        "orders_with_category = orders_df.withColumn('demand_level', demand_expr)\n",
        "orders_with_category.orderBy('order_date', 'region').show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean Up\n",
        "\n",
        "Always stop the SparkSession when your notebook run is complete.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "- Write a comprehension that filters the `regions` list to only coastal entries and prints the result.\n",
        "- Implement a helper function that normalizes region names (uppercase and trimmed) before building the Spark DataFrame.\n",
        "- Save your Python helpers to a separate `.py` module and import them back into the notebook to reinforce reuse patterns.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
