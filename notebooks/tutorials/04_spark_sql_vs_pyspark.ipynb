{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Spark SQL vs. PySpark DataFrame API\n",
        "\n",
        "Compare the Spark SQL interface with the PySpark DataFrame API so you can choose the right tool for each task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "- Spark environment capable of running PySpark notebooks.\n",
        "- Shared dataset `notebooks/data/orders_demo.csv` available.\n",
        "- Familiarity with basic SQL and the PySpark DataFrame API.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load DataFrame and Create a View\n",
        "\n",
        "We load the shared orders dataset once, then create a temporary view for SQL queries. This mirrors typical production jobs that support both Python and SQL entry points.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from pyspark.sql import SparkSession, functions as F\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkSQLvsPySpark').getOrCreate()\n",
        "\n",
        "repo_root = Path.cwd()\n",
        "if (repo_root / 'notebooks').exists():\n",
        "    data_path = repo_root / 'notebooks' / 'data' / 'orders_demo.csv'\n",
        "else:\n",
        "    data_path = Path('..') / 'data' / 'orders_demo.csv'\n",
        "\n",
        "orders_df = (\n",
        "    spark.read\n",
        "    .option('header', True)\n",
        "    .option('inferSchema', True)\n",
        "    .csv(str(data_path))\n",
        "    .withColumn('order_date', F.to_date('order_date'))\n",
        ")\n",
        "\n",
        "orders_df.createOrReplaceTempView('orders')\n",
        "orders_df.orderBy('order_date', 'region').show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Selecting Columns and Filtering Rows\n",
        "\n",
        "Both APIs can express the same logic. DataFrame code stays in Python, while Spark SQL keeps the query in SQL syntax.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# PySpark DataFrame API\n",
        "filtered_df = (\n",
        "    orders_df\n",
        "      .select('order_date', 'region', 'orders')\n",
        "      .where((F.col('region') == 'north') & (F.col('orders') >= 12))\n",
        "      .orderBy('order_date')\n",
        ")\n",
        "filtered_df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Spark SQL equivalent\n",
        "spark.sql(\n",
        "    '''\n",
        "    SELECT order_date, region, orders\n",
        "    FROM orders\n",
        "    WHERE region = \"north\" AND orders >= 12\n",
        "    ORDER BY order_date\n",
        "    '''\n",
        ").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adding Derived Columns\n",
        "\n",
        "Conditionals and built-in functions are available in both interfaces. PySpark uses Python functions under `pyspark.sql.functions`; SQL uses expressions and CASE statements.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# PySpark DataFrame API\n",
        "pyspark_enriched = orders_df.withColumn(\n",
        "    'demand_level',\n",
        "    F.when(F.col('orders') >= 14, 'high').otherwise('steady')\n",
        ")\n",
        "pyspark_enriched.orderBy('order_date', 'region').show()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Spark SQL equivalent\n",
        "spark.sql(\n",
        "    '''\n",
        "    SELECT *,\n",
        "           CASE WHEN orders >= 14 THEN 'high' ELSE 'steady' END AS demand_level\n",
        "    FROM orders\n",
        "    ORDER BY order_date, region\n",
        "    '''\n",
        ").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Aggregations and Grouping\n",
        "\n",
        "Aggregations feel similar, but DataFrame calls chain methods, while SQL exposes the familiar GROUP BY clause.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# PySpark DataFrame API\n",
        "pyspark_summary = (\n",
        "    orders_df\n",
        "      .groupBy('region')\n",
        "      .agg(\n",
        "          F.sum('orders').alias('total_orders'),\n",
        "          F.avg('orders').alias('avg_orders'),\n",
        "      )\n",
        "      .orderBy('region')\n",
        ")\n",
        "pyspark_summary.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Spark SQL equivalent\n",
        "spark.sql(\n",
        "    '''\n",
        "    SELECT region,\n",
        "           SUM(orders) AS total_orders,\n",
        "           AVG(orders) AS avg_orders\n",
        "    FROM orders\n",
        "    GROUP BY region\n",
        "    ORDER BY region\n",
        "    '''\n",
        ").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## When to Use Each Interface\n",
        "\n",
        "- **PySpark DataFrame API** keeps logic in Python, making it easy to integrate with functions, tests, or dynamic query construction.\n",
        "- **Spark SQL** shines when analysts contribute SQL snippets or when you register views consumed by BI tools.\n",
        "- Mix both: build reusable DataFrame transformations, then expose them as views for downstream SQL consumers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean Up\n",
        "\n",
        "Stop the SparkSession when you finish experimentation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "- Recreate a DataFrame transformation of your choice using both the DataFrame API and Spark SQL, verifying the results match.\n",
        "- Add a parameterized filter that accepts a region name from Python and injects it safely into a SQL query.\n",
        "- Compare the physical plans (`explain(mode='formatted')`) for the DataFrame and SQL versions of the same query\u2014note any differences.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
