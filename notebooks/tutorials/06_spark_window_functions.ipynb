{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analyzing Data with Spark Window Functions\n",
        "\n",
        "Window functions let you compute running totals, rankings, and moving statistics across partitions of your data without collapsing rows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Data Load\n",
        "\n",
        "We reuse the shared orders dataset stored at `notebooks/data/orders_demo.csv`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from pyspark.sql import SparkSession, functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkWindowTutorial').getOrCreate()\n",
        "\n",
        "repo_root = Path.cwd()\n",
        "if (repo_root / 'notebooks').exists():\n",
        "    data_path = repo_root / 'notebooks' / 'data' / 'orders_demo.csv'\n",
        "else:\n",
        "    data_path = Path('..') / 'data' / 'orders_demo.csv'\n",
        "\n",
        "orders_df = (\n",
        "    spark.read\n",
        "    .option('header', True)\n",
        "    .option('inferSchema', True)\n",
        "    .csv(str(data_path))\n",
        ")\n",
        "orders_df = orders_df.withColumn('order_date', F.to_date('order_date'))\n",
        "orders_df.orderBy('order_date', 'region').show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining Window Specifications\n",
        "\n",
        "Window specs describe how rows are partitioned and ordered when calculating running metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "region_day_window = Window.partitionBy('region').orderBy('order_date').rowsBetween(Window.unboundedPreceding, 0)\n",
        "region_rank_window = Window.partitionBy('region').orderBy(F.desc('orders'))\n",
        "rolling_two_day = Window.partitionBy('region').orderBy('order_date').rowsBetween(-1, 0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running Totals per Region\n",
        "\n",
        "`rowsBetween(Window.unboundedPreceding, 0)` keeps the running tally up to the current row for each region."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "running_totals = orders_df.withColumn(\n",
        "    'regional_running_orders',\n",
        "    F.sum('orders').over(region_day_window),\n",
        ")\n",
        "running_totals.orderBy('order_date', 'region').show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rolling Two-Day Average\n",
        "\n",
        "A row-based frame lets you look at the current and previous day to smooth daily volume swings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "rolling_avg = orders_df.withColumn(\n",
        "    'orders_two_day_avg',\n",
        "    F.avg('orders').over(rolling_two_day),\n",
        ")\n",
        "rolling_avg.orderBy('order_date', 'region').show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ranking Days by Demand\n",
        "\n",
        "Ranking identifies peak demand days per region while preserving the original rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "ranked = orders_df.withColumn(\n",
        "    'demand_rank',\n",
        "    F.dense_rank().over(region_rank_window),\n",
        ")\n",
        "ranked.orderBy('region', 'demand_rank').show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean Up\n",
        "\n",
        "Stop the SparkSession when you are done working with the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "- Calculate a three-day trailing sum of orders for each region using a window frame.\n",
        "- Use `row_number` to identify the first day each region exceeded 12 orders.\n",
        "- Visualize the rolling averages with a simple `matplotlib` plot or describe how you would export the results for visualization elsewhere.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
