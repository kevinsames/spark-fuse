{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing Notebook Template\n",
    "\n",
    "Use this scaffold to build repeatable PySpark pipelines with logging, validations, and optional Delta rollbacks. Replace placeholders with your sources, business logic, and checks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edd0beb",
   "metadata": {},
   "source": [
    "## Notebook guidelines\n",
    "\n",
    "- Name notebooks in clear snake_case (e.g., `orders_enriched.ipynb` or `domain_orders_enriched.ipynb`); keep one primary table per notebook.\n",
    "- Make runs idempotent: deterministic transforms, safe overwrites/merges, and repeatable partition logic so reruns do not create duplicates.\n",
    "- Keep scopes clean: functions in snake_case, classes in PascalCase, constants UPPER_SNAKE, modules/files in snake_case; avoid implicit globals beyond parameters.\n",
    "- Encapsulate helpers in small functions inside the notebook when reusable; prefer pure functions and pass Spark/DataFrames explicitly.\n",
    "- Document inputs/outputs near the top and ensure the notebook owns producing one curated table/view, not many.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d8eb2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark_fuse.spark import create_session\n",
    "from spark_fuse.utils import change_tracking  # noqa: F401 (enables .change_tracking accessors)\n",
    "from spark_fuse.utils.logging import create_progress_tracker, enable_spark_logging, log_info as log_info_step, log_error as log_error_step, log_end as log_end_step, console\n",
    "from spark_fuse.utils.dataframe import ensure_columns, preview\n",
    "from pyspark.sql import functions as F, types as T\n",
    "import datetime as _dt\n",
    "\n",
    "progress_tracker = create_progress_tracker(total_steps=10)\n",
    "log = console()\n",
    "\n",
    "def log_info(label: str, *, advance: int = 1) -> None:\n",
    "    log_info_step(progress_tracker, log, label, advance=advance)\n",
    "\n",
    "\n",
    "def log_error(label: str, *, advance: int = 1) -> None:\n",
    "    log_error_step(progress_tracker, log, label, advance=advance)\n",
    "\n",
    "\n",
    "def log_end(label: str, *, advance: int = 1) -> None:\n",
    "    log_end_step(progress_tracker, log, label, advance=advance)\n",
    "\n",
    "# Set any reusable parameters here\n",
    "job_ts = _dt.datetime.now().replace(microsecond=0).isoformat()  # UTC timestamp; override if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9adf8d4",
   "metadata": {},
   "source": [
    "> Why `functions as F` and `types as T`? Aliasing keeps chained expressions concise, matches common Spark style, and avoids polluting the global namespace with hundreds of column functions and type classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f826fa91",
   "metadata": {},
   "source": [
    "## Create a session\n",
    "\n",
    "Adjust `app_name`, `master`, and configs for your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6746ecc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Spark session ready:  10%|█         | 1/10 [00:00<00:02,  3.24it/s, +0.31s, total 0.32s]"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>data-processing-template</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1208f4d10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_info(\"Starting Spark session...\", advance=0)\n",
    "spark = create_session(\n",
    "    app_name=\"data-processing-template\",\n",
    "    master=\"local[*]\",\n",
    "    extra_configs={\"spark.some.credential\": \"value\"},\n",
    ")\n",
    "log_info(\"Spark session ready\")\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d21cc1",
   "metadata": {},
   "source": [
    "## Start logging\n",
    "\n",
    "Raise Spark log verbosity while you iterate so shuffle and scheduler details show up in the driver logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91c70754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Logging configured:  10%|█         | 1/10 [00:00<00:02,  3.24it/s, +0.00s, total 0.34s]            "
     ]
    }
   ],
   "source": [
    "enable_spark_logging(spark, level=\"WARN\")\n",
    "log_info(\"Spark logging enabled at WARN.\", advance=0)\n",
    "log_info(\"Logging configured\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005c76b3",
   "metadata": {},
   "source": [
    "## Load relevant data\n",
    "\n",
    "Declare input locations and load dataframes. Swap formats and options for your sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb0ad257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Input data loaded:  20%|██        | 2/10 [00:00<00:02,  3.24it/s, +0.04s, total 0.39s]                                               "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Orders sample:\n",
      "rows=[{'order_id': 'O-1001', 'order_ts': '2024-01-05', 'order_total': 42.5, 'customer_id': 'C001'}, {'order_id': 'O-1002', 'order_ts': '2024-01-06', 'order_total': 18.0, 'customer_id': 'C002'}, {'order_id': 'O-1003', 'order_ts': '2024-01-06', 'order_total': 120.75, 'customer_id': 'C003'}]\n",
      "INFO: Orders sample:id:string,order_ts:string,order_total:double,customer_id:string>:  30%|███       | 3/10 [00:01<00:02,  3.24it/s, +0.04s, total 0.39s]\n",
      "rows=[{'order_id': 'O-1001', 'order_ts': '2024-01-05', 'order_total': 42.5, 'customer_id': 'C001'}, {'order_id': 'O-1002', 'order_ts': '2024-01-06', 'order_total': 18.0, 'customer_id': 'C002'}, {'order_id': 'O-1003', 'order_ts': '2024-01-06', 'order_total': 120.75, 'customer_id': 'C003'}]\n",
      "INFO: Orders sample:id:string,order_ts:string,order_total:double,customer_id:string>:  30%|███       | 3/10 [00:01<00:02,  3.24it/s, +1.05s, total 1.43s]\n",
      "rows=[{'order_id': 'O-1001', 'order_ts': '2024-01-05', 'order_total': 42.5, 'customer_id': 'C001'}, {'order_id': 'O-1002', 'order_ts': '2024-01-06', 'order_total': 18.0, 'customer_id': 'C002'}, {'order_id': 'O-1003', 'order_ts': '2024-01-06', 'order_total': 120.75, 'customer_id': 'C003'}]\n",
      "schema=struct<order_id:string,order_ts:string,order_total:double,customer_id:string>:  30%|███       | 3/10 [00:01<00:02,  3.24it/s, +1.05s, total 1.43s]"
     ]
    }
   ],
   "source": [
    "log_info(\"Loading input data (dummy samples; replace with real sources)...\", advance=0)\n",
    "\n",
    "orders_schema = T.StructType(\n",
    "    [\n",
    "        T.StructField(\"order_id\", T.StringType(), False),\n",
    "        T.StructField(\"order_ts\", T.StringType(), False),\n",
    "        T.StructField(\"order_total\", T.DoubleType(), False),\n",
    "        T.StructField(\"customer_id\", T.StringType(), False),\n",
    "    ]\n",
    ")\n",
    "orders_data = [\n",
    "    (\"O-1001\", \"2024-01-05\", 42.50, \"C001\"),\n",
    "    (\"O-1002\", \"2024-01-06\", 18.00, \"C002\"),\n",
    "    (\"O-1003\", \"2024-01-06\", 120.75, \"C003\"),\n",
    "]\n",
    "orders_df = spark.createDataFrame(orders_data, schema=orders_schema)\n",
    "\n",
    "customers_schema = T.StructType(\n",
    "    [\n",
    "        T.StructField(\"customer_id\", T.StringType(), False),\n",
    "        T.StructField(\"segment\", T.StringType(), True),\n",
    "        T.StructField(\"country\", T.StringType(), True),\n",
    "    ]\n",
    ")\n",
    "customers_data = [\n",
    "    (\"C001\", \"retail\", \"US\"),\n",
    "    (\"C002\", \"enterprise\", \"CA\"),\n",
    "    (\"C003\", \"retail\", \"UK\"),\n",
    "]\n",
    "customers_df = spark.createDataFrame(customers_data, schema=customers_schema)\n",
    "\n",
    "log_info(\"Input data loaded\")\n",
    "log_info(f\"Orders sample:\\n{preview(orders_df)}\", advance=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0c1c0b",
   "metadata": {},
   "source": [
    "## Process data\n",
    "\n",
    "Apply your business logic: filtering, casting, enrichment, and derived columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fed7489b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Curated datasets ready:  40%|████      | 4/10 [00:01<00:02,  2.70it/s, +0.00s, total 1.47s]                                                                                                                                                                                                                                                                                                                                                                               "
     ]
    }
   ],
   "source": [
    "log_info(\"Curating datasets...\", advance=0)\n",
    "curated_orders_df = (\n",
    "    orders_df\n",
    "    .withColumn(\"order_date\", F.to_date(\"order_ts\"))\n",
    "    .withColumn(\"order_month\", F.date_format(\"order_date\", \"yyyy-MM\"))\n",
    "    .withColumn(\"processing_ts\", F.lit(job_ts))\n",
    ")\n",
    "\n",
    "curated_customers_df = customers_df.select(\"customer_id\", \"segment\", \"country\")\n",
    "log_info(\"Curated orders and customers dataframes ready.\", advance=0)\n",
    "log_info(\"Curated datasets ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ff13d0",
   "metadata": {},
   "source": [
    "## Do joins\n",
    "\n",
    "Join curated datasets and pick the right join strategy for your domain (inner/left/anti).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab9ca99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Join complete. Sample:ts...:  40%|████      | 4/10 [00:01<00:02,  2.70it/s, +0.01s, total 1.48s]\n",
      "rows=[{'customer_id': 'C001', 'order_id': 'O-1001', 'order_ts': '2024-01-05', 'order_total': 42.5, 'order_date': datetime.date(2024, 1, 5), 'order_month': '2024-01', 'processing_ts': '2026-01-20T13:47:26', 'segment': 'retail', 'country': 'US'}, {'customer_id': 'C002', 'order_id': 'O-1002', 'order_ts': '2024-01-06', 'order_total': 18.0, 'order_date': datetime.date(2024, 1, 6), 'order_month': '2024-01', 'processing_ts': '2026-01-20T13:47:26', 'segment': 'enterprise', 'country': 'CA'}, {'customer_id': 'C003', 'order_id': 'O-1003', 'order_ts': '2024-01-06', 'order_total': 120.75, 'order_date': datetime.date(2024, 1, 6), 'order_month': '2024-01', 'processing_ts': '2026-01-20T13:47:26', 'segment': 'retail', 'country': 'UK'}]\n",
      "INFO: Join complete. Sample:ring,order_id:string,order_ts:string,order_total:double,order_date:date,order_month:string,processing_ts:string,segment:string,country:string>:  40%|████      | 4/10 [00:02<00:02,  2.70it/s, +0.01s, total 1.48s]\n",
      "rows=[{'customer_id': 'C001', 'order_id': 'O-1001', 'order_ts': '2024-01-05', 'order_total': 42.5, 'order_date': datetime.date(2024, 1, 5), 'order_month': '2024-01', 'processing_ts': '2026-01-20T13:47:26', 'segment': 'retail', 'country': 'US'}, {'customer_id': 'C002', 'order_id': 'O-1002', 'order_ts': '2024-01-06', 'order_total': 18.0, 'order_date': datetime.date(2024, 1, 6), 'order_month': '2024-01', 'processing_ts': '2026-01-20T13:47:26', 'segment': 'enterprise', 'country': 'CA'}, {'customer_id': 'C003', 'order_id': 'O-1003', 'order_ts': '2024-01-06', 'order_total': 120.75, 'order_date': datetime.date(2024, 1, 6), 'order_month': '2024-01', 'processing_ts': '2026-01-20T13:47:26', 'segment': 'retail', 'country': 'UK'}]\n",
      "INFO: Join complete. Sample:ring,order_id:string,order_ts:string,order_total:double,order_date:date,order_month:string,processing_ts:string,segment:string,country:string>:  40%|████      | 4/10 [00:02<00:02,  2.70it/s, +0.88s, total 2.36s]\n",
      "rows=[{'customer_id': 'C001', 'order_id': 'O-1001', 'order_ts': '2024-01-05', 'order_total': 42.5, 'order_date': datetime.date(2024, 1, 5), 'order_month': '2024-01', 'processing_ts': '2026-01-20T13:47:26', 'segment': 'retail', 'country': 'US'}, {'customer_id': 'C002', 'order_id': 'O-1002', 'order_ts': '2024-01-06', 'order_total': 18.0, 'order_date': datetime.date(2024, 1, 6), 'order_month': '2024-01', 'processing_ts': '2026-01-20T13:47:26', 'segment': 'enterprise', 'country': 'CA'}, {'customer_id': 'C003', 'order_id': 'O-1003', 'order_ts': '2024-01-06', 'order_total': 120.75, 'order_date': datetime.date(2024, 1, 6), 'order_month': '2024-01', 'processing_ts': '2026-01-20T13:47:26', 'segment': 'retail', 'country': 'UK'}]\n",
      "INFO: Join complete:  50%|█████     | 5/10 [00:02<00:02,  1.95it/s, +0.00s, total 2.36s]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "log_info(\"Joining curated datasets...\", advance=0)\n",
    "joined_df = (\n",
    "    curated_orders_df.alias(\"o\")\n",
    "    .join(curated_customers_df.alias(\"c\"), on=\"customer_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "log_info(f\"Join complete. Sample:\\n{preview(joined_df)}\", advance=0)\n",
    "log_info(\"Join complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bc70ca",
   "metadata": {},
   "source": [
    "## Do data tests\n",
    "\n",
    "Add lightweight checks so issues surface early during development.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "578d80e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: In-memory data tests passed:  60%|██████    | 6/10 [00:03<00:02,  1.55it/s, +0.96s, total 3.33s]    "
     ]
    }
   ],
   "source": [
    "log_info(\"Running in-memory data tests...\", advance=0)\n",
    "# Schema/column guardrails\n",
    "ensure_columns(joined_df, [\"order_id\", \"customer_id\", \"order_date\"])\n",
    "\n",
    "# Null/uniqueness/data quality checks (expand as needed)\n",
    "assert joined_df.filter(F.col(\"order_id\").isNull()).count() == 0, \"order_id should be populated\"\n",
    "assert joined_df.filter(F.col(\"customer_id\").isNull()).count() == 0, \"customer_id should be populated\"\n",
    "assert joined_df.dropDuplicates([\"order_id\"]).count() == joined_df.count(), \"order_id should be unique\"\n",
    "\n",
    "# Domain-specific rule example; swap column names for your metric\n",
    "invalid_states = joined_df.filter(F.col(\"order_total\") < 0).count()\n",
    "assert invalid_states == 0, f\"Found {invalid_states} negative order totals\"\n",
    "log_info(\"In-memory data tests passed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c235f35",
   "metadata": {},
   "source": [
    "## Write data & post-write tests\n",
    "\n",
    "Persist curated results, run post-write validations, and attempt rollback when Delta Lake is available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34435cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Write prerequisites complete:  60%|██████    | 6/10 [00:03<00:02,  1.55it/s, +0.00s, total 3.37s]                                    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/20 13:47:33 WARN MapPartitionsRDD: RDD 90 was locally checkpointed, its lineage has been truncated and cannot be recomputed after unpersisting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Post-write validations passed:  90%|█████████ | 9/10 [00:09<00:01,  1.48s/it, +1.04s, total 9.99s]                      "
     ]
    }
   ],
   "source": [
    "output_path = \"/tmp/spark_fuse/orders_enriched_ct\"  # dummy local path; replace with real target (e.g., s3://bucket/silver/orders)\n",
    "target_table = \"orders_enriched_ct\"  # metastore table name if you want one registered\n",
    "log_info(f\"Preparing to write dataset to {output_path}\", advance=0)\n",
    "\n",
    "delta_supported = False\n",
    "pre_write_version = None\n",
    "output_format = \"delta\"\n",
    "table_exists = False\n",
    "try:\n",
    "    from delta.tables import DeltaTable\n",
    "    delta_supported = True\n",
    "    try:\n",
    "        DeltaTable.forPath(spark, output_path)\n",
    "        table_exists = True\n",
    "        log_info(\"Existing Delta table found; skipping DDL.\", advance=0)\n",
    "    except Exception:\n",
    "        log_info(\"No existing Delta table found at output path; creating with change-tracking columns.\", advance=0)\n",
    "        (\n",
    "            DeltaTable.createIfNotExists(spark)\n",
    "            .tableName(target_table)\n",
    "            .location(output_path)\n",
    "            .addColumn(\"order_id\", T.StringType())\n",
    "            .addColumn(\"order_ts\", T.StringType())\n",
    "            .addColumn(\"order_total\", T.DoubleType())\n",
    "            .addColumn(\"customer_id\", T.StringType())\n",
    "            .addColumn(\"order_date\", T.DateType())\n",
    "            .addColumn(\"order_month\", T.StringType())\n",
    "            .addColumn(\"processing_ts\", T.StringType())\n",
    "            .addColumn(\"segment\", T.StringType())\n",
    "            .addColumn(\"country\", T.StringType())\n",
    "            .addColumn(\"effective_start_ts\", T.TimestampType())\n",
    "            .addColumn(\"effective_end_ts\", T.TimestampType())\n",
    "            .addColumn(\"is_current\", T.BooleanType())\n",
    "            .addColumn(\"version\", T.LongType())\n",
    "            .addColumn(\"row_hash\", T.StringType())\n",
    "            .addColumn(\"load_ts\", T.TimestampType())\n",
    "            .execute()\n",
    "        )\n",
    "except Exception:\n",
    "    output_format = \"parquet\"\n",
    "    log_info(\"Delta Lake not available; falling back to parquet for write and disabling rollback.\", advance=0)\n",
    "\n",
    "log_info(f\"Writing data to final path with format={output_format}...\", advance=0)\n",
    "log_info(\"Write prerequisites complete\")\n",
    "\n",
    "if delta_supported:\n",
    "    change_tracking_options = {\n",
    "        \"business_keys\": [\"order_id\"],\n",
    "        \"tracked_columns\": [\n",
    "            \"order_id\",\n",
    "            \"customer_id\",\n",
    "            \"order_date\",\n",
    "            \"order_month\",\n",
    "            \"processing_ts\",\n",
    "            \"segment\",\n",
    "            \"country\",\n",
    "            \"order_total\",\n",
    "        ],\n",
    "        \"load_ts_expr\": \"current_timestamp()\",\n",
    "        \"create_if_not_exists\": not table_exists,\n",
    "        \"allow_schema_evolution\": True,\n",
    "    }\n",
    "    joined_df.write.change_tracking.options(\n",
    "        change_tracking_mode=\"track_history\",\n",
    "        change_tracking_options=change_tracking_options,\n",
    "    ).table(output_path)\n",
    "else:\n",
    "    (\n",
    "        joined_df.write\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .mode(\"overwrite\")\n",
    "        .format(output_format)\n",
    "        .partitionBy(\"order_month\")\n",
    "        .save(output_path)\n",
    "    )\n",
    "\n",
    "log_info(\"Write complete\")\n",
    "\n",
    "log_info(\"Running post-write validations on persisted data...\", advance=0)\n",
    "persisted_df = spark.read.format(output_format).load(output_path)\n",
    "current_df = persisted_df.filter(F.col(\"is_current\") == F.lit(True)) if delta_supported else persisted_df\n",
    "\n",
    "try:\n",
    "    ensure_columns(current_df, [\"order_id\", \"customer_id\", \"order_date\", \"order_month\"])\n",
    "    assert current_df.count() > 0, \"Persisted dataset is empty\"\n",
    "    assert current_df.filter(F.col(\"order_id\").isNull()).count() == 0, \"order_id should be populated\"\n",
    "    assert current_df.dropDuplicates([\"order_id\"]).count() == current_df.count(), \"order_id should be unique\"\n",
    "    invalid_persisted_states = current_df.filter(F.col(\"order_total\") < 0).count()\n",
    "    assert invalid_persisted_states == 0, f\"Found {invalid_persisted_states} negative order totals after write\"\n",
    "    log_info(\"Post-write validations passed\")\n",
    "except Exception as exc:\n",
    "    log_error(f\"Post-write validation failed: {exc}\", advance=0)\n",
    "    if delta_supported and pre_write_version is not None:\n",
    "        try:\n",
    "            log_info(f\"Attempting Delta rollback to version {pre_write_version} ...\", advance=0)\n",
    "            spark.sql(f\"RESTORE TABLE delta.`{output_path}` TO VERSION AS OF {pre_write_version}\")\n",
    "            log_info(\"Rollback succeeded.\", advance=0)\n",
    "        except Exception as rollback_exc:\n",
    "            log_error(f\"Rollback attempt failed: {rollback_exc}\", advance=0)\n",
    "    else:\n",
    "        log_info(\"No rollback available; inspect persisted data manually.\", advance=0)\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09594d6a",
   "metadata": {},
   "source": [
    "## Post-write Delta log\n",
    "Recent Delta history after writing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82d8b0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|version|timestamp              |operation|operationParameters                                                                                                                                                                                                                           |operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "+-------+-----------------------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|11     |2026-01-20 13:47:34.44 |WRITE    |{mode -> Append, partitionBy -> []}                                                                                                                                                                                                           |{numFiles -> 1, numOutputRows -> 3, numOutputBytes -> 4814}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|10     |2026-01-20 13:47:31.667|MERGE    |{predicate -> [\"((order_id#3538 <=> order_id#3371) AND is_current#3549)\"], matchedPredicates -> [{\"predicate\":\"NOT (row_hash#3551 <=> row_hash#3491)\",\"actionType\":\"update\"}], notMatchedPredicates -> [], notMatchedBySourcePredicates -> []}|{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetBytesRemoved -> 4815, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 3, numTargetRowsMatchedDeleted -> 0, numTargetRowsUpdated -> 3, numTargetChangeFilesAdded -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 220, numTargetFilesAdded -> 1, numTargetBytesAdded -> 5104, executionTimeMs -> 1868, materializeSourceTimeMs -> 361, numTargetRowsInserted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 1283, numOutputRows -> 3, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numSourceRows -> 3, numTargetFilesRemoved -> 1}|\n",
      "|9      |2026-01-20 13:35:20.366|WRITE    |{mode -> Append, partitionBy -> []}                                                                                                                                                                                                           |{numFiles -> 1, numOutputRows -> 3, numOutputBytes -> 4815}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|8      |2026-01-20 13:35:18.649|MERGE    |{predicate -> [\"((order_id#167 <=> order_id#0) AND is_current#178)\"], matchedPredicates -> [{\"predicate\":\"NOT (row_hash#180 <=> row_hash#120)\",\"actionType\":\"update\"}], notMatchedPredicates -> [], notMatchedBySourcePredicates -> []}       |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetBytesRemoved -> 4816, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 3, numTargetRowsMatchedDeleted -> 0, numTargetRowsUpdated -> 3, numTargetChangeFilesAdded -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 409, numTargetFilesAdded -> 1, numTargetBytesAdded -> 5105, executionTimeMs -> 3965, materializeSourceTimeMs -> 744, numTargetRowsInserted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 2789, numOutputRows -> 3, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numSourceRows -> 3, numTargetFilesRemoved -> 1}|\n",
      "|7      |2026-01-20 13:22:15.529|WRITE    |{mode -> Append, partitionBy -> []}                                                                                                                                                                                                           |{numFiles -> 1, numOutputRows -> 3, numOutputBytes -> 4816}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|6      |2026-01-20 13:22:13.58 |MERGE    |{predicate -> [\"((order_id#167 <=> order_id#0) AND is_current#178)\"], matchedPredicates -> [{\"predicate\":\"NOT (row_hash#180 <=> row_hash#120)\",\"actionType\":\"update\"}], notMatchedPredicates -> [], notMatchedBySourcePredicates -> []}       |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetBytesRemoved -> 4816, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 3, numTargetRowsMatchedDeleted -> 0, numTargetRowsUpdated -> 3, numTargetChangeFilesAdded -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 409, numTargetFilesAdded -> 1, numTargetBytesAdded -> 5105, executionTimeMs -> 3815, materializeSourceTimeMs -> 503, numTargetRowsInserted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 2883, numOutputRows -> 3, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numSourceRows -> 3, numTargetFilesRemoved -> 1}|\n",
      "|5      |2026-01-20 12:53:37.219|WRITE    |{mode -> Append, partitionBy -> []}                                                                                                                                                                                                           |{numFiles -> 1, numOutputRows -> 3, numOutputBytes -> 4816}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|4      |2026-01-20 12:53:14.618|MERGE    |{predicate -> [\"((order_id#3538 <=> order_id#3371) AND is_current#3549)\"], matchedPredicates -> [{\"predicate\":\"NOT (row_hash#3551 <=> row_hash#3491)\",\"actionType\":\"update\"}], notMatchedPredicates -> [], notMatchedBySourcePredicates -> []}|{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetBytesRemoved -> 4815, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 3, numTargetRowsMatchedDeleted -> 0, numTargetRowsUpdated -> 3, numTargetChangeFilesAdded -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 189, numTargetFilesAdded -> 1, numTargetBytesAdded -> 5104, executionTimeMs -> 1646, materializeSourceTimeMs -> 275, numTargetRowsInserted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 1179, numOutputRows -> 3, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numSourceRows -> 3, numTargetFilesRemoved -> 1}|\n",
      "|3      |2026-01-20 12:52:16.513|WRITE    |{mode -> Append, partitionBy -> []}                                                                                                                                                                                                           |{numFiles -> 1, numOutputRows -> 3, numOutputBytes -> 4815}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|2      |2026-01-20 12:52:15.175|MERGE    |{predicate -> [\"((order_id#167 <=> order_id#0) AND is_current#178)\"], matchedPredicates -> [{\"predicate\":\"NOT (row_hash#180 <=> row_hash#120)\",\"actionType\":\"update\"}], notMatchedPredicates -> [], notMatchedBySourcePredicates -> []}       |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetBytesRemoved -> 4816, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 3, numTargetRowsMatchedDeleted -> 0, numTargetRowsUpdated -> 3, numTargetChangeFilesAdded -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 298, numTargetFilesAdded -> 1, numTargetBytesAdded -> 5105, executionTimeMs -> 3585, materializeSourceTimeMs -> 491, numTargetRowsInserted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 2777, numOutputRows -> 3, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numSourceRows -> 3, numTargetFilesRemoved -> 1}|\n",
      "+-------+-----------------------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "+-------+---------------+-----------------------+-------------+-------------+-------------------+---------------------+-------------------------+-----------------------------+-------------------------------+-------------------------------+-------------------+---------------------+-------------------+--------------------+---------------------+---------------------------+---------------------------+--------------------------------------+--------------------------------------+--------------------+-------------+----------+\n",
      "|version|executionTimeMs|materializeSourceTimeMs|numOutputRows|numSourceRows|numTargetBytesAdded|numTargetBytesRemoved|numTargetChangeFilesAdded|numTargetDeletionVectorsAdded|numTargetDeletionVectorsRemoved|numTargetDeletionVectorsUpdated|numTargetFilesAdded|numTargetFilesRemoved|numTargetRowsCopied|numTargetRowsDeleted|numTargetRowsInserted|numTargetRowsMatchedDeleted|numTargetRowsMatchedUpdated|numTargetRowsNotMatchedBySourceDeleted|numTargetRowsNotMatchedBySourceUpdated|numTargetRowsUpdated|rewriteTimeMs|scanTimeMs|\n",
      "+-------+---------------+-----------------------+-------------+-------------+-------------------+---------------------+-------------------------+-----------------------------+-------------------------------+-------------------------------+-------------------+---------------------+-------------------+--------------------+---------------------+---------------------------+---------------------------+--------------------------------------+--------------------------------------+--------------------+-------------+----------+\n",
      "|2      |3585           |491                    |3            |3            |5105               |4816                 |0                        |0                            |0                              |0                              |1                  |1                    |0                  |0                   |0                    |0                          |3                          |0                                     |0                                     |3                   |298          |2777      |\n",
      "|4      |1646           |275                    |3            |3            |5104               |4815                 |0                        |0                            |0                              |0                              |1                  |1                    |0                  |0                   |0                    |0                          |3                          |0                                     |0                                     |3                   |189          |1179      |\n",
      "|6      |3815           |503                    |3            |3            |5105               |4816                 |0                        |0                            |0                              |0                              |1                  |1                    |0                  |0                   |0                    |0                          |3                          |0                                     |0                                     |3                   |409          |2883      |\n",
      "|8      |3965           |744                    |3            |3            |5105               |4816                 |0                        |0                            |0                              |0                              |1                  |1                    |0                  |0                   |0                    |0                          |3                          |0                                     |0                                     |3                   |409          |2789      |\n",
      "|10     |1868           |361                    |3            |3            |5104               |4815                 |0                        |0                            |0                              |0                              |1                  |1                    |0                  |0                   |0                    |0                          |3                          |0                                     |0                                     |3                   |220          |1283      |\n",
      "+-------+---------------+-----------------------+-------------+-------------+-------------------+---------------------+-------------------------+-----------------------------+-------------------------------+-------------------------------+-------------------+---------------------+-------------------+--------------------+---------------------+---------------------------+---------------------------+--------------------------------------+--------------------------------------+--------------------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import functions as F\n",
    "log_path = Path(output_path) / '_delta_log'\n",
    "if delta_supported and log_path.exists():\n",
    "    try:\n",
    "        dt = DeltaTable.forPath(spark, output_path)\n",
    "        history_df = dt.history(10)\n",
    "        merge_ops = history_df.filter(F.col('operation') == 'MERGE')\n",
    "        history_df.select('version','timestamp','operation','operationParameters','operationMetrics').show(truncate=False)\n",
    "        pivoted = (\n",
    "            merge_ops\n",
    "            .select('version', F.explode('operationMetrics').alias('metric','value'))\n",
    "            .where(F.col('value').isNotNull())\n",
    "            .groupBy('version')\n",
    "            .pivot('metric')\n",
    "            .agg(F.first('value'))\n",
    "            .orderBy('version')\n",
    "        )\n",
    "        pivoted.show(truncate=False)\n",
    "    except Exception as exc:\n",
    "        log_info(f\"Delta history not available: {exc}\", advance=0)\n",
    "else:\n",
    "    log_info(\"No Delta log found after write; ensure output_path is correct.\", advance=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb1b198",
   "metadata": {},
   "source": [
    "## Stop session\n",
    "\n",
    "Shut down the session once the job completes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de9e3c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END: Spark session stopped: 100%|██████████| 10/10 [00:11<00:00,  1.13s/it, +0.81s, total 11.29s]       \n"
     ]
    }
   ],
   "source": [
    "log_info(\"Stopping Spark session.\", advance=0)\n",
    "spark.stop()\n",
    "log_end(\"Spark session stopped\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
