{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Self-Contained Data Processing Template\n",
        "\n",
        "Standalone PySpark scaffold (no spark_fuse imports) to create a session, log progress, load dummy data, transform, test, and write results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notebook guidelines\n",
        "\n",
        "- Keep one primary table per notebook; name notebooks in snake_case.\n",
        "- Make runs idempotent: deterministic transforms, safe overwrites/merges, repeatable partition logic.\n",
        "- Functions in snake_case, classes in PascalCase, constants UPPER_SNAKE; avoid global state.\n",
        "- Document inputs/outputs; prefer pure helpers defined inline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/gc/x7zs83sd6_vcbd1p6t67kj8c0000gn/T/ipykernel_29845/1251055435.py:35: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  job_ts = _dt.datetime.utcnow().replace(microsecond=0).isoformat()\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import types as T\n",
        "import datetime as _dt\n",
        "import time\n",
        "\n",
        "# Lightweight helpers\n",
        "def create_progress_tracker(total_steps: int):\n",
        "    return {\"current\": 0, \"total\": float(total_steps), \"start\": time.perf_counter(), \"last\": None}\n",
        "\n",
        "def log_progress(tracker, label: str):\n",
        "    now = time.perf_counter()\n",
        "    last = tracker[\"last\"] or tracker[\"start\"]\n",
        "    tracker[\"current\"] += 1\n",
        "    tracker[\"last\"] = now\n",
        "    current = int(tracker[\"current\"])\n",
        "    total = tracker[\"total\"] or 1\n",
        "    elapsed = now - last\n",
        "    total_elapsed = now - tracker[\"start\"]\n",
        "    filled = max(0, min(10, int(10 * current / total)))\n",
        "    bar = \"#\" * filled + \".\" * (10 - filled)\n",
        "    print(f\"[INFO] [{bar}] {current}/{int(total)} {label} (+{elapsed:.2f}s, total {total_elapsed:.2f}s)\")\n",
        "\n",
        "def ensure_columns(df, required):\n",
        "    missing = [c for c in required if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing required columns: {missing}\")\n",
        "    return df\n",
        "\n",
        "def preview(df, n=5):\n",
        "    rows = [r.asDict(recursive=True) for r in df.limit(n).collect()]\n",
        "    print(f\"rows={rows}\\nschema={df.schema.simpleString()}\")\n",
        "\n",
        "progress_tracker = create_progress_tracker(total_steps=8)\n",
        "job_ts = _dt.datetime.utcnow().replace(microsecond=0).isoformat()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a Spark session\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Using incubator modules: jdk.incubator.vector\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Starting Spark session...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/12/02 09:22:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] [#.........] 1/8 Spark session ready (+2.46s, total 2.46s)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://air-von-kevin.fritz.box:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v4.0.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>data-processing-template-standalone</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x1329ec920>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"[INFO] Starting Spark session...\")\n",
        "builder = (\n",
        "    SparkSession.builder.appName(\"data-processing-template-standalone\")\n",
        "    .master(\"local[*]\")\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
        ")\n",
        "\n",
        "# Optional: add Delta extensions if packages are available\n",
        "builder = builder.config(\n",
        "    \"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\"\n",
        ")\n",
        "\n",
        "spark = builder.getOrCreate()\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "log_progress(progress_tracker, \"Spark session ready\")\n",
        "spark\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load dummy data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Input data loaded.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rows=[{'order_id': 'O-1001', 'order_ts': '2024-01-05', 'order_total': 42.5, 'customer_id': 'C001'}, {'order_id': 'O-1002', 'order_ts': '2024-01-06', 'order_total': 18.0, 'customer_id': 'C002'}, {'order_id': 'O-1003', 'order_ts': '2024-01-06', 'order_total': 120.75, 'customer_id': 'C003'}]\n",
            "schema=struct<order_id:string,order_ts:string,order_total:double,customer_id:string>\n",
            "[INFO] [##........] 2/8 Input data loaded (+1.72s, total 4.17s)\n"
          ]
        }
      ],
      "source": [
        "orders_schema = T.StructType(\n",
        "    [\n",
        "        T.StructField(\"order_id\", T.StringType(), False),\n",
        "        T.StructField(\"order_ts\", T.StringType(), False),\n",
        "        T.StructField(\"order_total\", T.DoubleType(), False),\n",
        "        T.StructField(\"customer_id\", T.StringType(), False),\n",
        "    ]\n",
        ")\n",
        "orders_data = [\n",
        "    (\"O-1001\", \"2024-01-05\", 42.50, \"C001\"),\n",
        "    (\"O-1002\", \"2024-01-06\", 18.00, \"C002\"),\n",
        "    (\"O-1003\", \"2024-01-06\", 120.75, \"C003\"),\n",
        "]\n",
        "orders_df = spark.createDataFrame(orders_data, schema=orders_schema)\n",
        "\n",
        "customers_schema = T.StructType(\n",
        "    [\n",
        "        T.StructField(\"customer_id\", T.StringType(), False),\n",
        "        T.StructField(\"segment\", T.StringType(), True),\n",
        "        T.StructField(\"country\", T.StringType(), True),\n",
        "    ]\n",
        ")\n",
        "customers_data = [\n",
        "    (\"C001\", \"retail\", \"US\"),\n",
        "    (\"C002\", \"enterprise\", \"CA\"),\n",
        "    (\"C003\", \"retail\", \"UK\"),\n",
        "]\n",
        "customers_df = spark.createDataFrame(customers_data, schema=customers_schema)\n",
        "\n",
        "print(\"[INFO] Input data loaded.\")\n",
        "preview(orders_df)\n",
        "log_progress(progress_tracker, \"Input data loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Process data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Curated datasets ready.\n",
            "[INFO] [###.......] 3/8 Curated datasets ready (+0.05s, total 4.23s)\n"
          ]
        }
      ],
      "source": [
        "curated_orders_df = (\n",
        "    orders_df\n",
        "    .withColumn(\"order_date\", F.to_date(\"order_ts\"))\n",
        "    .withColumn(\"order_month\", F.date_format(\"order_date\", \"yyyy-MM\"))\n",
        "    .withColumn(\"processing_ts\", F.lit(job_ts))\n",
        ")\n",
        "curated_customers_df = customers_df.select(\"customer_id\", \"segment\", \"country\")\n",
        "print(\"[INFO] Curated datasets ready.\")\n",
        "log_progress(progress_tracker, \"Curated datasets ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Join\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Join complete.\n",
            "rows=[{'customer_id': 'C001', 'order_id': 'O-1001', 'order_ts': '2024-01-05', 'order_total': 42.5, 'order_date': datetime.date(2024, 1, 5), 'order_month': '2024-01', 'processing_ts': '2025-12-02T08:21:59', 'segment': 'retail', 'country': 'US'}, {'customer_id': 'C002', 'order_id': 'O-1002', 'order_ts': '2024-01-06', 'order_total': 18.0, 'order_date': datetime.date(2024, 1, 6), 'order_month': '2024-01', 'processing_ts': '2025-12-02T08:21:59', 'segment': 'enterprise', 'country': 'CA'}, {'customer_id': 'C003', 'order_id': 'O-1003', 'order_ts': '2024-01-06', 'order_total': 120.75, 'order_date': datetime.date(2024, 1, 6), 'order_month': '2024-01', 'processing_ts': '2025-12-02T08:21:59', 'segment': 'retail', 'country': 'UK'}]\n",
            "schema=struct<customer_id:string,order_id:string,order_ts:string,order_total:double,order_date:date,order_month:string,processing_ts:string,segment:string,country:string>\n",
            "[INFO] [#####.....] 4/8 Join complete (+0.57s, total 4.80s)\n"
          ]
        }
      ],
      "source": [
        "joined_df = (\n",
        "    curated_orders_df.alias(\"o\")\n",
        "    .join(curated_customers_df.alias(\"c\"), on=\"customer_id\", how=\"left\")\n",
        ")\n",
        "print(\"[INFO] Join complete.\")\n",
        "preview(joined_df)\n",
        "log_progress(progress_tracker, \"Join complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data tests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Running data tests...\n",
            "[INFO] Data tests passed.\n",
            "[INFO] [######....] 5/8 In-memory data tests passed (+1.07s, total 5.87s)\n"
          ]
        }
      ],
      "source": [
        "print(\"[INFO] Running data tests...\")\n",
        "ensure_columns(joined_df, [\"order_id\", \"customer_id\", \"order_date\", \"order_month\"])\n",
        "assert joined_df.filter(F.col(\"order_id\").isNull()).count() == 0, \"order_id should be populated\"\n",
        "assert joined_df.filter(F.col(\"customer_id\").isNull()).count() == 0, \"customer_id should be populated\"\n",
        "assert joined_df.dropDuplicates([\"order_id\"]).count() == joined_df.count(), \"order_id should be unique\"\n",
        "invalid_states = joined_df.filter(F.col(\"order_total\") < 0).count()\n",
        "assert invalid_states == 0, f\"Found {invalid_states} negative order totals\"\n",
        "print(\"[INFO] Data tests passed.\")\n",
        "log_progress(progress_tracker, \"In-memory data tests passed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Write data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Writing data to /tmp/spark_fuse/orders_enriched_standalone (Parquet)...\n",
            "[INFO] [#######...] 6/8 Write complete (+0.96s, total 6.83s)\n"
          ]
        }
      ],
      "source": [
        "output_path = \"/tmp/spark_fuse/orders_enriched_standalone\"\n",
        "print(f\"[INFO] Writing data to {output_path} (Parquet)...\")\n",
        "(\n",
        "    joined_df.write\n",
        "    .mode(\"overwrite\")\n",
        "    .format(\"parquet\")\n",
        "    .partitionBy(\"order_month\")\n",
        "    .save(output_path)\n",
        ")\n",
        "log_progress(progress_tracker, \"Write complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Post-write validations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Post-write validations passed.\n",
            "[INFO] [########..] 7/8 Post-write validations passed (+0.60s, total 7.43s)\n"
          ]
        }
      ],
      "source": [
        "persisted_df = spark.read.parquet(output_path)\n",
        "ensure_columns(persisted_df, [\"order_id\", \"customer_id\", \"order_date\", \"order_month\"])\n",
        "assert persisted_df.count() > 0, \"Persisted dataset is empty\"\n",
        "assert persisted_df.filter(F.col(\"order_id\").isNull()).count() == 0, \"order_id should be populated\"\n",
        "assert persisted_df.dropDuplicates([\"order_id\"]).count() == persisted_df.count(), \"order_id should be unique\"\n",
        "invalid_persisted_states = persisted_df.filter(F.col(\"order_total\") < 0).count()\n",
        "assert invalid_persisted_states == 0, f\"Found {invalid_persisted_states} negative order totals after write\"\n",
        "print(\"[INFO] Post-write validations passed.\")\n",
        "log_progress(progress_tracker, \"Post-write validations passed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stop session\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Stopping Spark session.\n",
            "[INFO] Spark session stopped.\n",
            "[INFO] [##########] 8/8 Spark session stopped (+0.38s, total 7.81s)\n"
          ]
        }
      ],
      "source": [
        "print(\"[INFO] Stopping Spark session.\")\n",
        "spark.stop()\n",
        "print(\"[INFO] Spark session stopped.\")\n",
        "log_progress(progress_tracker, \"Spark session stopped\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.12.3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
