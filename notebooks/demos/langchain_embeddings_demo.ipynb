{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Databricks LangChain Embeddings Demo\n\nThis notebook shows how to attach vector embeddings to a Spark DataFrame using `spark_fuse.utils.llm.with_langchain_embeddings` together with LangChain's `DatabricksEmbeddings`.\n\n## Prerequisites\n- Packages: `spark-fuse`, `pyspark`, `langchain-core`, `langchain-community`, `langchain-text-splitters`.\n- Databricks credentials: set `DATABRICKS_HOST` and `DATABRICKS_TOKEN` (or configure a Databricks profile).\n- A served embedding endpoint name such as `databricks-bge-large-en` (Model Serving) or another model your workspace exposes.\n\nTo avoid external calls while prototyping, switch to the stub embeddings class at the bottom of the notebook.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from databricks_langchain import DatabricksEmbeddings\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom spark_fuse.spark import create_session\nfrom spark_fuse.utils.llm import with_langchain_embeddings\n\nspark = create_session(app_name=\"spark-fuse-dbx-embeddings-demo\", master=\"local[2]\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample data\n",
    "\n",
    "We will embed a small set of product descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"id\": 1, \"text\": \"A lightweight hiking backpack with 20L capacity.\"},\n",
    "    {\"id\": 2, \"text\": \"Insulated stainless steel water bottle, 750ml.\"},\n",
    "    {\"id\": 3, \"text\": \"Breathable running shoes for road and trail.\"},\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed with DatabricksEmbeddings\n",
    "\n",
    "Use a factory (`lambda: DatabricksEmbeddings(...)`) so each executor initializes its own client. Configure `DATABRICKS_HOST`/`DATABRICKS_TOKEN` and point to your embedding endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded = with_langchain_embeddings(\n",
    "    df,\n",
    "    input_col=\"text\",\n",
    "    embeddings=lambda: DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\"),\n",
    "    output_col=\"embedding\",\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "embedded.select(\"id\", \"embedding\").show(3, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk long documents with a text splitter\n",
    "\n",
    "When documents are long, add a LangChain splitter. Chunk embeddings are combined with the chosen aggregation strategy (`mean` below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=64, chunk_overlap=16)\n",
    "\n",
    "split_embedded = with_langchain_embeddings(\n",
    "    df,\n",
    "    input_col=\"text\",\n",
    "    embeddings=lambda: DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\"),\n",
    "    text_splitter=splitter,\n",
    "    aggregation=\"mean\",\n",
    "    output_col=\"embedding_mean\",\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "split_embedded.select(\"id\", \"embedding_mean\").show(3, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline or cost-free testing with a stub\n",
    "\n",
    "For quick validation without network calls, define a minimal embeddings class. This keeps the schema and workflow identical while producing deterministic vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StubEmbeddings:\n",
    "    \"\"\"Deterministic hash-based embeddings for local testing.\"\"\"\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        def _vec(text):\n",
    "            # Map length and simple checksum into a tiny vector for demonstration only.\n",
    "            length = float(len(text))\n",
    "            checksum = float(sum(ord(ch) for ch in text) % 97)\n",
    "            return [length, checksum]\n",
    "\n",
    "        return [_vec(t) for t in texts]\n",
    "\n",
    "\n",
    "stubbed = with_langchain_embeddings(\n",
    "    df,\n",
    "    input_col=\"text\",\n",
    "    embeddings=StubEmbeddings(),\n",
    "    output_col=\"embedding_stub\",\n",
    "    batch_size=4,\n",
    ")\n",
    "\n",
    "stubbed.select(\"id\", \"embedding_stub\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "Stop the SparkSession when finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
